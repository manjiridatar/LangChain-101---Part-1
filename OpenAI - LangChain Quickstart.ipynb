{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd27b67c",
   "metadata": {},
   "source": [
    "# LangChain - Quickstart Guide - The Basics - Part 1 (LLM)\n",
    "\n",
    "### LangChain is a framework which simplifies the development of apps using LLMs like OpenAI's GPT-4\n",
    "\n",
    "##### Quickstart Guide location = https://python.langchain.com/en/latest/getting_started/getting_started.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af654da",
   "metadata": {},
   "source": [
    "### Install LangChain and OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c66eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c296bba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2475b33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install google-search-results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "187c36e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f80d1fc",
   "metadata": {},
   "source": [
    "#### We need to set the environment variable \"OPENAI_API_KEY\", so lets import os and then set the key it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fda34e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdf1f379",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"SERPAPI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3dadb4",
   "metadata": {},
   "source": [
    "#### Import the OpenAI LLM wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8796d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0636bf72",
   "metadata": {},
   "source": [
    "### LLM:  The most basic building block of LangChain is called llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7e9f1c",
   "metadata": {},
   "source": [
    "#### Here we call the llm with the temperature argument set to a HIGH value (0.9) for more random values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2131cb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature = 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb47ea7",
   "metadata": {},
   "source": [
    "#### Lets set the input text as a prompt to the llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65a8473b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Please list 5 vacation destination that are best for hikers\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd99c774",
   "metadata": {},
   "source": [
    "#### Now lets call the model with this text and print the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97e879f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Grand Canyon National Park, Arizona\n",
      "2. Olympic National Park, Washington\n",
      "3. Machu Picchu, Peru\n",
      "4. Yosemite National Park, California\n",
      "5. Torres del Paine National Park, Chile\n"
     ]
    }
   ],
   "source": [
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda3b80a",
   "metadata": {},
   "source": [
    "### PromptTemplate: Managing Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737eb6aa",
   "metadata": {},
   "source": [
    "#### PromptTemplates can be used to set a generic prompt, which can then be used to construct the actual prompt text for the llm based on user input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267cc8d6",
   "metadata": {},
   "source": [
    "#### Let's import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c3e338f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27bf81e",
   "metadata": {},
   "source": [
    "#### Now lets contruct the prompt we want to pass to the llm using PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50f34c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "            input_variables = [\"profession\"],\n",
    "            template = \"what are the 5 vacation destination for a {profession}?\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8041462e",
   "metadata": {},
   "source": [
    "#### Construct a prompt for the llm using the PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a93409d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what are the 5 vacation destination for a historian?\n"
     ]
    }
   ],
   "source": [
    "print(prompt.format(profession = \"historian\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24db03b0",
   "metadata": {},
   "source": [
    "#### Call the llm with the constructed prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bebd65d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Rome, Italy\n",
      "2. Athens, Greece\n",
      "3. Cairo, Egypt\n",
      "4. Jerusalem, Israel\n",
      "5. Washington D.C., USA\n"
     ]
    }
   ],
   "source": [
    "print(llm(prompt.format(profession = \"historian\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f970a8c8",
   "metadata": {},
   "source": [
    "#### Call the llm with a different prompt constructed using the same PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4571aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Paris, France\n",
      "2. Bali, Indonesia\n",
      "3. Rome, Italy\n",
      "4. Bangkok, Thailand\n",
      "5. New York City, USA\n"
     ]
    }
   ],
   "source": [
    "print(llm(prompt.format(profession = \"baker\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b68c1e",
   "metadata": {},
   "source": [
    "### Chains - Combine LLMs and PromptTemplates into a multi-step workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9d61daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e15b5850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n1. Yosemite National Park, California, USA\\n2. Patagonia, Argentina\\n3. Chamonix, France\\n4. Kalymnos, Greece\\n5. Korouoma, Finland'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"Rock Climber\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e548b0",
   "metadata": {},
   "source": [
    "### Agents - Dynamically call Chains based on user inputs\n",
    "\n",
    "If we want to use the llm to determine which action to take and in what order, we use agents\n",
    "An action can either be using a tool and observing its output, or returning to the user.\n",
    "A tool is a function that performs a specified function. This can be things like: Google Search, Database lookup, Python REPL, other chains. \n",
    "\n",
    "Agents: For a list of supported agents and their specifications, https://python.langchain.com/en/latest/modules/agents/agents.html\n",
    "\n",
    "Tools: For a list of predefined tools and their specifications, https://python.langchain.com/en/latest/modules/agents/tools.html.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02af92bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8bdb240f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the llm with low temperature\n",
    "\n",
    "llm = OpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fcf0b5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets load the tools we will use for this purpose\n",
    "# tools = load_tools([\"serpapi\", \"llm-math\"], llm = llm)\n",
    "tools = load_tools([\"serpapi\"], llm = llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba5f3b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's initalize an agent wit the tools that we just loaded, the language model and the type of agent we plan to use\n",
    "\n",
    "agent = initialize_agent(tools, llm, agent = AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "777b7b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find out what the temperature was in NYC yesterday\n",
      "Action: Search\n",
      "Action Input: \"High temperature in NYC yesterday in Celcius\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mHigh & Low Weather Summary for the Past Weeks ; 54 °F · 71% ; * Reported Apr 22 12:51 pm — May 7 12:51 pm, New York.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to convert the temperature from Fahrenheit to Celcius\n",
      "Action: Search\n",
      "Action Input: \"Fahrenheit to Celcius conversion\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mFor a 100% accurate answer, subtract 32 and divide by 1.8 (or use the calculator above!) Fahrenheit to Celsius table.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: 12.8 °C\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'12.8 °C'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's run the agent\n",
    "\n",
    "agent.run(\"What was the high temperature in NYC yesterday, in Celcius?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1405aa3d",
   "metadata": {},
   "source": [
    "### Memory - Add State to Chains and Agents\n",
    "\n",
    "If our application requires our chains and agent to have memory of the previous conversation we use ConversationChain.\n",
    "A ConversationChain has a simple type of memory that remembers all previous inputs/outputs and adds them to the context that is passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "564a1b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hello!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " Hi there! It's nice to meet you. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "from langchain import OpenAI, ConversationChain\n",
    "\n",
    "llm = OpenAI(temperature = 0)\n",
    "conversation = ConversationChain(llm = llm, verbose = True)\n",
    "\n",
    "output = conversation.predict(input = \"Hello!\")\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a04dc7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hello!\n",
      "AI:  Hi there! It's nice to meet you. How can I help you today?\n",
      "Human: I'm doing awesome! What a beautiful day it is!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " Yes, it's a great day! The sun is shining and the birds are singing. It's a perfect day to get outside and enjoy the fresh air.\n"
     ]
    }
   ],
   "source": [
    "output = conversation.predict(input=\"I'm doing awesome! What a beautiful day it is!\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c2b4cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
